{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, namedtuple\n",
    "from text_generator import TextGenerator\n",
    "import gzip\n",
    "import cPickle as pickle\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sb\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb.set_context('poster', rc={'figure.figsize':(12, 12)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BiGramModel(object):\n",
    "\n",
    "\n",
    "    def __init__(self, transcripts, target_period=3, max_vocab_size=None, min_count=50, verbose=True):\n",
    "\n",
    "        # Initiate some necessary variables\n",
    "        self._transcripts = transcripts\n",
    "        self._vocab = defaultdict(dict)\n",
    "        self._features = list()\n",
    "        self._targets = list()\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_count = min_count\n",
    "        self.stop_words = \"a an the for which our by as we that on in with or is are also and of to you these \" \\\n",
    "                          \"from at last where will i now how have be per during about these this - was year \" \\\n",
    "                          \"were call it has us than so not like very but million quarter over new first \" \\\n",
    "                          \"third their would those there . ? ! , & % $ [ ] ( ) ; and/or your 's 're 'd 've 'll\".split()\n",
    "\n",
    "        # Set the target period\n",
    "        assert target_period in (3, 30, 60, 90), 'Invalid target period: {} days'.format(target_period)\n",
    "        self.target_period = target_period\n",
    "\n",
    "        # Build the vocabulary\n",
    "        self.build_vocab(verbose)\n",
    "        if self.max_vocab_size:\n",
    "            self.finalize_vocab()\n",
    "        self.TFIDF()\n",
    "\n",
    "\n",
    "    def build_vocab(self, verbose):\n",
    "\n",
    "        for key, transcript in self._transcripts.iteritems():\n",
    "\n",
    "            abnormal_return = self.get_return(transcript)\n",
    "\n",
    "            if abnormal_return < -0.05:\n",
    "                self._targets.append(abnormal_return)\n",
    "            elif -0.05 <= abnormal_return < 0.05:\n",
    "                self._targets.append(abnormal_return)\n",
    "            elif abnormal_return >= 0.05:\n",
    "                self._targets.append(abnormal_return)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Tracks how often words appear in the transcript\n",
    "            tracker = defaultdict(int)\n",
    "\n",
    "            for paragraph in TextGenerator(transcript):\n",
    "\n",
    "                # Remembers last word for bigrams\n",
    "                last_word = None\n",
    "\n",
    "                for word in paragraph:\n",
    "\n",
    "                    # Ignore stop words\n",
    "                    if word in self.stop_words:\n",
    "                        continue\n",
    "\n",
    "                    # Ignore numbers\n",
    "                    try:\n",
    "                        float(word)\n",
    "                        continue\n",
    "                    except ValueError:\n",
    "                        if last_word:\n",
    "                            tracker[' '.join([last_word, word])] += 1\n",
    "                        tracker[word] += 1\n",
    "                        last_word = word\n",
    "\n",
    "            for word in tracker:\n",
    "                self._vocab[word][key] = tracker[word]\n",
    "\n",
    "            if verbose:\n",
    "                if key % 250 == 0:\n",
    "                    print(\"PROGRESS: at transcript #{}, keeping {} word types\".format(key, len(self._vocab)))\n",
    "\n",
    "                if key % 1000 == 0:\n",
    "                    self.trim_vocab(min_reduce=2)\n",
    "\n",
    "\n",
    "    def TFIDF(self):\n",
    "\n",
    "        print(\"Computing TF-IDF...\",)\n",
    "        # Compute inverse document frequency\n",
    "        N = len(self._transcripts)\n",
    "        idf = defaultdict(float)\n",
    "        for word in self._vocab:\n",
    "            freq = len(self._vocab[word])\n",
    "            idf[word] = np.log(float(N) / freq)\n",
    "\n",
    "        # Reaugment dictionary so transcripts are first-order keys and words are second-order keys\n",
    "        # Also set self._vocab to now just be the actual vocabulary\n",
    "        vocab = self._vocab.keys()\n",
    "        self._features = {doc: {word: self._vocab[word][doc] for word in self._vocab if doc in self._vocab[word]}\n",
    "                          for doc in self._transcripts}\n",
    "        self._vocab = vocab\n",
    "\n",
    "        for doc in self._features:\n",
    "            max_freq = float(max(self._features[doc].values()))\n",
    "            self._features[doc] = {word: ((count / max_freq) * 0.5 + 0.5) * idf[word] for word, count in\n",
    "                                 self._features[doc].iteritems()}\n",
    "        print(\"Done!\")\n",
    "\n",
    "\n",
    "    def trim_vocab(self, min_reduce):\n",
    "\n",
    "        \"\"\" Trims all words that appear less than min_reduce times \"\"\"\n",
    "\n",
    "        start = len(self._vocab)\n",
    "        for word in self._vocab.keys():\n",
    "            if sum(self._vocab[word].values()) < min_reduce:\n",
    "                del self._vocab[word]\n",
    "        print(\"trimmed {} word types, {} word types remaining\".format(start - len(self._vocab), len(self._vocab)))\n",
    "\n",
    "\n",
    "    def finalize_vocab(self):\n",
    "\n",
    "        while self.max_vocab_size and len(self._vocab) > self.max_vocab_size:\n",
    "            self.trim_vocab(min_reduce=self.min_count)\n",
    "            self.min_count += np.sqrt(np.sqrt(len(self.vocab)))\n",
    "\n",
    "        print(\"collected {} word types\".format(len(self._vocab)))\n",
    "\n",
    "\n",
    "    def get_return(self, transcript):\n",
    "\n",
    "        if self.target_period == 3:\n",
    "            return transcript.return_3days\n",
    "        elif self.target_period == 30:\n",
    "            return transcript.return_30days\n",
    "        elif self.target_period == 60:\n",
    "            return transcript.return_60days\n",
    "        else:\n",
    "            return transcript.return_90days\n",
    "\n",
    "\n",
    "    @property\n",
    "    def transcripts(self):\n",
    "        return self._transcripts\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self._vocab\n",
    "\n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return self._features\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Transcript = namedtuple('Transcript', ['company',       \n",
    "                                       'ticker',        \n",
    "                                       'date',          \n",
    "                                       'return_3days',  \n",
    "                                       'return_30days', \n",
    "                                       'return_60days', \n",
    "                                       'return_90days', \n",
    "                                       'prepared',      \n",
    "                                       'QandA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_transcripts():                                            \n",
    "    with gzip.open('../data/transcripts.p.gz') as f:         \n",
    "        return pickle.load(f)                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transcripts = load_transcripts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: at transcript #250, keeping 273457 word types\n",
      "PROGRESS: at transcript #500, keeping 477205 word types\n",
      "PROGRESS: at transcript #750, keeping 645082 word types\n",
      "PROGRESS: at transcript #1000, keeping 804645 word types\n",
      "trimmed 579036 word types, 225609 word types remaining"
     ]
    }
   ],
   "source": [
    "m = BiGramModel(transcripts, target_period=3, max_vocab_size=None, min_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.asarray([np.asarray([doc[word] if word in doc else 0 for word in m.vocab]) for doc in m.inputs.values()])\n",
    "y = np.asarray(m.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(x), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(y, n_iter=1, test_size=0.25, random_state=0)\n",
    "for train_index, test_index in sss:\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_clf.fit(x_train, y_train)\n",
    "y_pred = rf_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importances = zip(m.vocab, np.absolute(rf_clf.feature_importances_))\n",
    "importances = sorted(importances, key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb.barplot(y=zip(*importances)[0][:25], x=zip(*importances)[1][:25], color='g', orient='h');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_clf.fit(x_train, y_train)\n",
    "y_pred = lr_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importances = zip(m.vocab, np.sum(np.absolute(lr_clf.coef_), axis=0))\n",
    "importances = sorted(importances, key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb.barplot(y=zip(*importances)[0][:25], x=zip(*importances)[1][:25], color='g', orient='h');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
